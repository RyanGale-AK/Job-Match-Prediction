lasso.mod=glmnet(X_train,Ytrain2,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.lasso)^2)
lasso_mseB[i] = mean((Y_te2-pred.lasso)^2)
}
boxplot(ridge_mseB,lasso_mseB)
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain2,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain2,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
ridge_mseB[i] = mean((Y_te2-pred.ridge)^2)
}
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain2 = X_train%*%A + eps_train
Y_te2 = X_te%*%A + eps_te
cv.out=cv.glmnet(X_train,Ytrain2,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain2,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.lasso)^2)
lasso_mseB[i] = mean((Y_te2-pred.lasso)^2)
}
boxplot(ridge_mseB,lasso_mseB)
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain2,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain2,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
ridge_mseB[i] = mean((Y_te2-pred.ridge)^2)
}
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
cv.out=cv.glmnet(X_train,Ytrain2,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain2,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.lasso)^2)
lasso_mseB[i] = mean((Y_te2-pred.lasso)^2)
}
boxplot(ridge_mseB,lasso_mseB)
library(glmnet)
A = c(rep(2,5),rep(0,45))
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
p = 50
N = 100
set.seed(1)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
grid = 10^seq(10,-2,length = 100)
library(glmnet)
A = c(rep(2,5),rep(0,45))
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.ridge)^2)
ridge_mseA = rep(0,50)
ridge_mseA[1] = mean((Y_te1-pred.ridge)^2)
library(glmnet)
A = c(rep(2,5),rep(0,45))
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.ridge)^2)
ridge_mseA = rep(0,50)
ridge_mseA[1] = mean((Y_te1-pred.ridge)^2)
library(glmnet)
A = c(rep(2,5),rep(0,45))
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.ridge)^2)
ridge_mseA = rep(0,50)
ridge_mseA[1] = mean((Y_te1-pred.ridge)^2)
p = 50
N = 100
set.seed(1)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
grid = 10^seq(10,-2,length = 100)
library(glmnet)
A = c(rep(2,5),rep(0,45))
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.ridge)^2)
ridge_mseA = rep(0,50)
ridge_mseA[1] = mean((Y_te1-pred.ridge)^2)
cv.out=cv.glmnet(X_train,Ytrain1,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain1,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.lasso)^2)
lasso_mseA = rep(0,50)
lasso_mseA[1] = mean((Y_te1-pred.lasso)^2)
B = rep(0.5,50)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain2,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain2,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.ridge)^2)
ridge_mseB = rep(0,50)
ridge_mseB[1] = mean((Y_te2-pred.ridge)^2)
cv.out=cv.glmnet(X_train,Ytrain2,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain2,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.lasso)^2)
lasso_mseB = rep(0,50)
lasso_mseB[1] = mean((Y_te2-pred.lasso)^2)
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
ridge_mseA[i] = mean((Y_te1-pred.ridge)^2)
}
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
cv.out=cv.glmnet(X_train,Ytrain1,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain1,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.lasso)^2)
lasso_mseA[i] = mean((Y_te1-pred.lasso)^2)
}
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain2,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain2,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
ridge_mseB[i] = mean((Y_te2-pred.ridge)^2)
}
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
cv.out=cv.glmnet(X_train,Ytrain2,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain2,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.lasso)^2)
lasso_mseB[i] = mean((Y_te2-pred.lasso)^2)
}
boxplot(ridge_mseA,lasso_mseA)
boxplot(ridge_mseB,lasso_mseB)
X = [[4,1,1,1],[1,1,0,0],[1,0,1,0],[1,0,0,1]]
X = ((4,1,1,1),(1,1,0,0),(1,0,1,0),(1,0,0,1))
X = rbind(c(4,1,1,1),c(1,1,0,0),c(1,0,1,0),c(1,0,0,1))
X = rbind(c(4,1,1,1),c(1,1,0,0),c(1,0,1,0),c(1,0,0,1))
Y = solve(X)
Y
choose(5,2)
1-choose(1024-2,32)/choose(1024,32)
1-choose(1024-3,32)/choose(1024,32)
1-choose(1024-1,32)/choose(1024,32)
1-choose(1024-5,32)/choose(1024,32)
1-choose(1024-6,32)/choose(1024,32)
1-choose(1024-7,32)/choose(1024,32)
1-choose(1024-20,32)/choose(1024,32)
2(1/0.06155303) + 3(1/0.09093689) + 3(1/0.09093689) + 20(1/0.1998094) + 7(1/0.4732545) + 1(1/0.03125) + 6(1/0.1738397) + 5(1/0.1470544)+ 5(1/0.1470544) + 2(1/0.06155303) + 3(1/0.09093689)
2(1/0.06155303) + 3(1/0.09093689) + 3(1/0.09093689) + 20(1/0.1998094) + 7(1/0.4732545) + 1(1/0.03125) + 6(1/0.1738397) + 5(1/0.1470544)+ 5(1/0.1470544) + 2(1/0.06155303) + 3(1/0.09093689)
2*(1/0.06155303) + 3*(1/0.09093689) + 3*(1/0.09093689) + 20*(1/0.1998094) + 7*(1/0.4732545) + 1*(1/0.03125) + 6*(1/0.1738397) + 5*(1/0.1470544)+ 5*(1/0.1470544) + 2*(1/0.06155303) + 3*(1/0.09093689)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
View(Nutr)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
lm(BetaPlasma~. ,data = Nutr)
View(Nutr)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
L1 = lm(BetaPlasma~. ,data = Nutr)
model.matrix(L1)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
L1 = lm(BetaPlasma~. ,data = Nutr)
L1
model.matrix(L1)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(BetaPlasma~. ,data = Nutr)
a.lm
M = model.matrix(a.lm)
M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%M
View(M)
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
M%*%t(Li)
t(Li)
Li
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
M%*%Li
#vitamin3 coef = vitamin2 coef /2
#Lii =
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
M%*%Li
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
M%*%Lii
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%coef(a.lm)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
M%*%Lii
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%coef(a.lm)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Lii%*%coef(a.lm)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(BetaPlasma~ . ,data = log(Nutr))
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(log(BetaPlasma)~ .,data = Nutr)
a.lm
M = model.matrix(a.lm)
M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%coef(a.lm)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Lii%*%coef(a.lm)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
contrasts = list(Vitamin = "contr.helmert")
View(contrasts)
View(contrasts)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
Li%*%coef(b.lm)
Lii%*%coef(b.lm)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
coef(b.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
View(L)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
Anova(a.lm)
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
variance = L%*%solve()
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=Nur[,2:7]
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=Nutr[,2:7]
View(Nutr)
View(Nutr)
View(Nutr)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(log(BetaPlasma)~ .,data = Nutr)
a.lm
M = model.matrix(a.lm)
M
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
(t(L%*%B)%*%solve(variance)%*%LB)/MSE
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
(t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
(t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
F = (t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
F/2
Anova(a.lm)
library(car)
Anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
F = (t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
F/2
L%*%B
Anova(a.lm)
Anova(b.lm)
Li%*%coef(a.lm)
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
coef(b.lm)
Anova(a.lm)
Anova(b.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
F = (t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
F/2
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(log(BetaPlasma)~ .,data = Nutr)
a.lm
M = model.matrix(a.lm)
M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
coef(b.lm)
library(car)
Anova(a.lm)
Anova(b.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
F = (t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
F/2
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Lii
source('~/Documents/Git/Job-Match-Prediction/lasso.R')
mean(cv.error)
mean_error = mean(cv.error)
