<<<<<<< HEAD
X = model.matrix(Y ~ ., data = data)[,-1]
library("boot")
library("glmnet")
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
job_data = read.csv("dtm_jobs.csv", header = T)
X = job_data[,10:9577]
X = as.data.frame(apply(X, 2, as.factor))
Y = as.factor(job_data$Category)
#Y2 = lapply(Y1, as.factor)
data = cbind(X, Y)
x = model.matrix(Y ~ ., data = data)[,-1]
set.seed(1)
grid = 10^seq(10,-2,length = 100)
lasso.mod = glmnet(x,Y,alpha = 1, lambda = .1)
x
###Provided code
setwd("C:/Users/Jeffrey Wu/Downloads")
p = 50
###Provided code
setwd("C:/Users/Jeffrey Wu/Downloads")
=======
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.ridge)^2)
ridge_mseA = rep(0,50)
ridge_mseA[1] = mean((Y_te1-pred.ridge)^2)
library(glmnet)
A = c(rep(2,5),rep(0,45))
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.ridge)^2)
ridge_mseA = rep(0,50)
ridge_mseA[1] = mean((Y_te1-pred.ridge)^2)
library(glmnet)
A = c(rep(2,5),rep(0,45))
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.ridge)^2)
ridge_mseA = rep(0,50)
ridge_mseA[1] = mean((Y_te1-pred.ridge)^2)
>>>>>>> 84fa75361c3cea7561078cba140fc1010675cc23
p = 50
N = 100
set.seed(1)
X_train = array(rnorm(p*N),c(N,p))
epsilon_train = rnorm(N)
Ntest = 10^3
X_test = array(rnorm(p*Ntest),c(Ntest,p))
epsilon_test = rnorm(Ntest)
grid = 10^seq(10,-2,length = 100)
<<<<<<< HEAD
A = c(rep(2,5), rep(0,45))
###Generate training data
Y_train = rep(1,100)
Y_train = X_train %*% A + epsilon_train
###Generate test data
Y_test = rep(0,1000)
Y_test = X_test %*% A + epsilon_test
typeof(X_train)
typeof(Y_train)
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
job_data = read.csv("dtm_jobs.csv", header = T)
X = job_data[,10:9577]
X = as.matrix(apply(X, 2, as.factor))
library("boot")
library("glmnet")
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
job_data = read.csv("dtm_jobs.csv", header = T)
X = job_data[,10:9577]
X = apply(X, 2, as.factor)
X = lapply(X, 2, as.factor)
X = lapply(X, as.factor)
library("boot")
library("glmnet")
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
job_data = read.csv("dtm_jobs.csv", header = T)
X = job_data[,10:9577]
X = lapply(X, as.factor)
X = as.matrix(lapply(X, as.factor))
Y = as.factor(job_data$Category)
data = cbind(X, Y)
x = model.matrix(Y ~ ., data = data)[,-1]
X = as.data.frame(lapply(X, as.factor))
Y = as.factor(job_data$Category)
data = cbind(X, Y)
x = model.matrix(Y ~ ., data = data)[,-1]
set.seed(1)
grid = 10^seq(10,-2,length = 100)
lasso.mod = glmnet(x,Y,alpha = 1, lambda = .1)
library(tree)
install.packages("tree")
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
###Tree method code
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
X = Carseats
X = Carseats[-Sales]
View(Carseats)
X = Carseats[CompPrice]
X = Carseats[CompPrice]
X = Carseats,CompPrice:US]
X = Carseats[CompPrice:US]
Y = Carseats$High
X = Carseats$CompPrice
Y = Carseats$High
tree.carseats=tree(X,Y)
tree.carseats=tree(Y~X)
summary(tree.carseats)
plot(tree.carseats)
X = c(Carseats$CompPrice, Carseats$Income)
Y = Carseats$High
tree.carseats=tree(Y~X)
data = data.frame(X,Y)
tree.carseats=tree(Y~X,data)
summary(tree.carseats)
plot(tree.carseats)
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
data = read.csv("dtm_jobs.csv", header = T)
library("boot")
library("glmnet")
library("caret")
library("matrixStats")
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
library("boot")
library("glmnet")
library("caret")
install.packages("caret")
library("matrixStats")
install.packages("matrixStats")
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
job_data = read.csv("dtm_jobs.csv", header = T)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
install.packages("tree")
###Tree method code
library(tree)
library(ISLR)
data = data.frame(X,Y)
library("boot")
library("glmnet")
library("caret")
library("matrixStats")
setwd("~/Documents/Git/Job-Match-Prediction")
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
job_data = read.csv("dtm_jobs.csv", header = T)
job_data = read.csv("dtm_jobs.csv", header = T)
library("boot")
library("glmnet")
library("caret")
library("matrixStats")
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
###Tree method code
library(tree)
library(ISLR)
data = data.frame(X,Y)
tree.data = tree(Y~X,data)
summary(tree.data)
plot(tree.carseats)
plot(tree.data)
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
install.packages("randomForest")
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
Boston
library(ISLR)
Boston
library(MASS)
Boston
Boston = Boston
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
library(randomForest)
library(randomForest)
library(MASS)
Boston = Boston
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
library(randomForest)
library(MASS)
Boston = Boston
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
cv.tree = cv.tree(tree.data)
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
job_data = read.csv("dtm_jobs.csv", header = T)
library("boot")
library("glmnet")
library("caret")
library("matrixStats")
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
library(tree)
library(ISLR)
data = data.frame(X,Y)
tree.data = tree(Y~X,data)
summary(tree.data)
plot(tree.data)
cv.tree = cv.tree(tree.data)
plot(cv.tree)
library(tree)
library(ISLR)
data = data.frame(X,Y)
tree.job = tree(Y~X,data)
summary(tree.job)
plot(tree.job)
cv.tree.job = cv.tree.job(tree.job)
prune.job = prune.tree(tree.job,best=5)
cv.tree.job = cv.tree(tree.job)
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
job_data = read.csv("dtm_jobs.csv", header = T)
library("boot")
library("glmnet")
library("caret")
library("matrixStats")
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
###Tree method code
library(tree)
library(ISLR)
data = data.frame(X,Y)
tree.job = tree(Y~X,data)
summary(tree.job)
plot(tree.job)
cv.tree.job = cv.tree(tree.job)
prune.job = prune.tree(tree.job,best=5)
train = sample(nrow(data), (nrow(data))/2)
job.test = data[-train,"medv"]
job.test = data[-train,]
library(randomForest)
bag.job = randomForest(Y~X, data = data, subset = train, mtry = 13, importance = TRUE)
bag.job
bag.job = randomForest(Y~X, data = data, subset = train, mtry = 13, ntree = 25)
bag.job = randomForest(Y~X, data = data, mtry = 13, importance = TRUE)
help("randomForest")
bag.job = randomForest(Y~X, data = data, subset = train, mtry = 100, importance = TRUE)
rf.job = randomForest(Y~X, data = data, subset = train, mtry = 50, importance = TRUE)
setwd("C:/Users/Jeffrey Wu/Desktop/STSCI 4740/Final project")
job_data = read.csv("dtm_jobs.csv", header = T)
library("boot")
library("glmnet")
library("caret")
library("matrixStats")
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
train = sample(nrow(data), (nrow(data))/2)
job.test = data[-train,]
train = sample(nrow(data), (nrow(data))/2)
library(tree)
library(ISLR)
data = data.frame(X,Y)
train = sample(nrow(data), (nrow(data))/2)
job.test = data[-train,]
library(randomForest)
bag.job = randomForest(Y~X, data = data, subset = train, mtry = 50, importance = TRUE)
tree.job = tree(Y~X, data = data, subset = train)
summary(tree.job)
plot(tree.job)
cv.tree.job = cv.tree(tree.job)
tree.job = tree(Y~X, data = data)
summary(tree.job)
plot(tree.job)
cv.tree.job = cv.tree(tree.job)
prune.job = prune.tree(tree.job,best=5)
###Tree method code
setwd("~/Documents/Git/Job-Match-Prediction")
###Tree method code
setwd("C:/Users/Jeffrey Wu/Desktop/GitHub/Job-Match-Prediction")
job_data = read.csv("dtm_jobs.csv", header =TRUE)
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
library(tree)
data = data.frame(X,Y)
tree.job = tree(Y~X, data = data)
summary(tree.job)
plot(tree.job)
cv.tree.job = cv.tree(tree.job)
prune.job = prune.tree(tree.job,best=5)
prune.job
plot(prune.job)
cv.tree.job
###Tree method code
setwd("~/Documents/Git/Job-Match-Prediction")
###Tree method code
#setwd("~/Documents/Git/Job-Match-Prediction")
job_data = read.csv("dtm_jobs.csv", header =TRUE)
###Tree method code
#setwd("~/Documents/Git/Job-Match-Prediction")
job_data = read.csv("dtm_jobs.csv", header =TRUE)
summary(prune.job)
###Tree method code
#setwd("~/Documents/Git/Job-Match-Prediction")
job_data = read.csv("dtm_jobs.csv", header =TRUE)
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
library(tree)
data = data.frame(X,Y)
tree.job = tree(Y~X, data = data)
summary(tree.job)
plot(tree.job)
cv.tree.job = cv.tree(tree.job)
prune.job = prune.tree(tree.job,best=5)
prune.job
plot(prune.job)
summary(prune.job)
cv.tree.job
plot(cv.tree.job)
###Tree method code
#setwd("~/Documents/Git/Job-Match-Prediction")
job_data = read.csv("dtm_jobs.csv", header =TRUE)
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Category)
library(tree)
data = data.frame(X,Y)
tree.job = tree(Y~X, data = data)
summary(tree.job)
plot(tree.job)
job_data = read.csv("dtm_jobs.csv", header =TRUE)
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Category)
library(tree)
data = data.frame(X,Y)
train = sample(nrow(data), (nrow(data))/2)
job.test = data[-train,]
library(randomForest)
bag.job = randomForest(Y~X, data = data, subset = train, mtry = 50, importance = TRUE)
bag.job = randomForest(Y~X, data = data, subset = train, mtry = 5, importance = TRUE)
bag.job = randomForest(Y~X, data = data, mtry = 50, importance = TRUE)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
bag.boston
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
rf.boston
install.packages("gbm")
###Tree method code
#setwd("~/Documents/Git/Job-Match-Prediction")
=======
library(glmnet)
A = c(rep(2,5),rep(0,45))
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.ridge)^2)
ridge_mseA = rep(0,50)
ridge_mseA[1] = mean((Y_te1-pred.ridge)^2)
cv.out=cv.glmnet(X_train,Ytrain1,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain1,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.lasso)^2)
lasso_mseA = rep(0,50)
lasso_mseA[1] = mean((Y_te1-pred.lasso)^2)
B = rep(0.5,50)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain2,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain2,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.ridge)^2)
ridge_mseB = rep(0,50)
ridge_mseB[1] = mean((Y_te2-pred.ridge)^2)
cv.out=cv.glmnet(X_train,Ytrain2,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain2,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.lasso)^2)
lasso_mseB = rep(0,50)
lasso_mseB[1] = mean((Y_te2-pred.lasso)^2)
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain1,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain1,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
ridge_mseA[i] = mean((Y_te1-pred.ridge)^2)
}
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain1 = X_train%*%A + eps_train
Y_te1 = X_te%*%A + eps_te
cv.out=cv.glmnet(X_train,Ytrain1,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain1,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te1-pred.lasso)^2)
lasso_mseA[i] = mean((Y_te1-pred.lasso)^2)
}
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
#alpha=0 is the ridge penalty, alpha=1 is the lasso penalty
cv.out=cv.glmnet(X_train,Ytrain2,alpha=0,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
ridge.mod=glmnet(X_train,Ytrain2,alpha=0,lambda=bestlam)
coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s = bestlam, newx = X_te)
ridge_mseB[i] = mean((Y_te2-pred.ridge)^2)
}
for(i in 2:50){
set.seed(i)
X_train = array(rnorm(p*N),c(N,p))
eps_train = rnorm(N)
Nte = 10^3
X_te = array(rnorm(p*Nte),c(Nte,p))
eps_te = rnorm(Nte)
Ytrain2 = X_train%*%B + eps_train
Y_te2 = X_te%*%B + eps_te
cv.out=cv.glmnet(X_train,Ytrain2,alpha=1,lambda=grid) #10 fold cross validation
bestlam=cv.out$lambda.min
#bestlam
lasso.mod=glmnet(X_train,Ytrain2,alpha=1,lambda=bestlam)
coef(lasso.mod)[,1]
pred.lasso = predict(lasso.mod, s = bestlam, newx = X_te)
mean((Y_te2-pred.lasso)^2)
lasso_mseB[i] = mean((Y_te2-pred.lasso)^2)
}
boxplot(ridge_mseA,lasso_mseA)
boxplot(ridge_mseB,lasso_mseB)
X = [[4,1,1,1],[1,1,0,0],[1,0,1,0],[1,0,0,1]]
X = ((4,1,1,1),(1,1,0,0),(1,0,1,0),(1,0,0,1))
X = rbind(c(4,1,1,1),c(1,1,0,0),c(1,0,1,0),c(1,0,0,1))
X = rbind(c(4,1,1,1),c(1,1,0,0),c(1,0,1,0),c(1,0,0,1))
Y = solve(X)
Y
choose(5,2)
1-choose(1024-2,32)/choose(1024,32)
1-choose(1024-3,32)/choose(1024,32)
1-choose(1024-1,32)/choose(1024,32)
1-choose(1024-5,32)/choose(1024,32)
1-choose(1024-6,32)/choose(1024,32)
1-choose(1024-7,32)/choose(1024,32)
1-choose(1024-20,32)/choose(1024,32)
2(1/0.06155303) + 3(1/0.09093689) + 3(1/0.09093689) + 20(1/0.1998094) + 7(1/0.4732545) + 1(1/0.03125) + 6(1/0.1738397) + 5(1/0.1470544)+ 5(1/0.1470544) + 2(1/0.06155303) + 3(1/0.09093689)
2(1/0.06155303) + 3(1/0.09093689) + 3(1/0.09093689) + 20(1/0.1998094) + 7(1/0.4732545) + 1(1/0.03125) + 6(1/0.1738397) + 5(1/0.1470544)+ 5(1/0.1470544) + 2(1/0.06155303) + 3(1/0.09093689)
2*(1/0.06155303) + 3*(1/0.09093689) + 3*(1/0.09093689) + 20*(1/0.1998094) + 7*(1/0.4732545) + 1*(1/0.03125) + 6*(1/0.1738397) + 5*(1/0.1470544)+ 5*(1/0.1470544) + 2*(1/0.06155303) + 3*(1/0.09093689)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
View(Nutr)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
lm(BetaPlasma~. ,data = Nutr)
View(Nutr)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
L1 = lm(BetaPlasma~. ,data = Nutr)
model.matrix(L1)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
L1 = lm(BetaPlasma~. ,data = Nutr)
L1
model.matrix(L1)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(BetaPlasma~. ,data = Nutr)
a.lm
M = model.matrix(a.lm)
M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%M
View(M)
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
M%*%t(Li)
t(Li)
Li
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
M%*%Li
#vitamin3 coef = vitamin2 coef /2
#Lii =
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
M%*%Li
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
M%*%Lii
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%coef(a.lm)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
M%*%Lii
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%coef(a.lm)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Lii%*%coef(a.lm)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(BetaPlasma~ . ,data = log(Nutr))
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(log(BetaPlasma)~ .,data = Nutr)
a.lm
M = model.matrix(a.lm)
M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li%*%coef(a.lm)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Lii%*%coef(a.lm)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
contrasts = list(Vitamin = "contr.helmert")
View(contrasts)
View(contrasts)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
Li%*%coef(b.lm)
Lii%*%coef(b.lm)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
coef(b.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
View(L)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
Anova(a.lm)
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
variance = L%*%solve()
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=Nur[,2:7]
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=Nutr[,2:7]
View(Nutr)
View(Nutr)
View(Nutr)
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(log(BetaPlasma)~ .,data = Nutr)
a.lm
M = model.matrix(a.lm)
M
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
(t(L%*%B)%*%solve(variance)%*%LB)/MSE
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
(t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
(t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
F = (t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
F/2
Anova(a.lm)
library(car)
Anova(a.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
F = (t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
F/2
L%*%B
Anova(a.lm)
Anova(b.lm)
Li%*%coef(a.lm)
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
coef(b.lm)
Anova(a.lm)
Anova(b.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
F = (t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
F/2
setwd("~/Downloads/")
Nutr = read.csv("NutritionStdy.csv")
Nutr$Vitamin = as.factor(Nutr$Vitamin)
a.lm = lm(log(BetaPlasma)~ .,data = Nutr)
a.lm
M = model.matrix(a.lm)
M
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Li%*%coef(a.lm)
Lii%*%coef(a.lm)
b.lm = lm(log(BetaPlasma)~ .,data = Nutr, contrasts = list(Vitamin = "contr.helmert"))
coef(b.lm)
library(car)
Anova(a.lm)
Anova(b.lm)
L= rbind(c(0,0,1,0,0,0,0),c(0,0,-1/2,1,0,0,0))
B= coef(a.lm)
X=model.matrix(a.lm)
variance = L%*%solve(t(X)%*%X)%*%t(L)
MSE = 140.207/307
F = (t(L%*%B)%*%solve(variance)%*%L%*%B)/MSE
F/2
#test vitamin2 coef = 0
Li = c(0,0,1,0,0,0,0)
Li
#vitamin3 coef = vitamin2 coef /2
Lii = c(0,0,-1/2,1,0,0,0)
Lii
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
job.test
Y[train]
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
mean(cv.error)
###Tree method code
library(tree)
library(randomForest)
setwd("~/Documents/Git/Job-Match-Prediction")
>>>>>>> 84fa75361c3cea7561078cba140fc1010675cc23
job_data = read.csv("dtm_jobs.csv", header =TRUE)
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
<<<<<<< HEAD
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
A = job_data[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Category)
library(tree)
data = data.frame(X,Y)
train = sample(nrow(data), (nrow(data))/2)
job.test = data[-train,]
library(gbm)
set.seed(1)
boost.job = gbm(Y~X, data = data[train,], distribution = "gaussian", n.trees = 500,
interaction.depth = 4)
boost.job = gbm(Y~X, data = data[-train,], distribution = "gaussian", n.trees = 500,
interaction.depth = 4)
source('C:/Users/Jeffrey Wu/Desktop/GitHub/Job-Match-Prediction/Tree.R')
source('C:/Users/Jeffrey Wu/Desktop/GitHub/Job-Match-Prediction/Tree.R')
###Tree method code
#setwd("~/Documents/Git/Job-Match-Prediction")
job_data = read.csv("dtm_jobs.csv", header =TRUE)
set.seed(1)
#Randomly shuffle the data
shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
A = shuffledData[,11:1814]
X= as.matrix(A)
Y = as.vector(shuffledData$Cat1)
library(tree)
data = data.frame(Y,X)
setwd("~/Documents/Git/Job-Match-Prediction")
job_data = read.csv("dtm_jobs.csv", header =TRUE)
set.seed(1)
#Randomly shuffle the data
#shuffledData<-job_data[sample(nrow(job_data)),]
#Create 10 equally size folds
#folds <- cut(seq(1,nrow(shuffledData)),breaks=5,labels=FALSE)
#X = shuffledData[,11:1814]
X = job_data[,11:1814]
Y = as.vector(job_data$Category)
Y = as.factor(Y)
data = data.frame(Y,X)
# tree.job = tree(Y~X, data = data)
# summary(tree.job)
# plot(tree.job)
#
# cv.tree.job = cv.tree(tree.job)
# prune.job = prune.tree(tree.job,best=5)
train = sample(nrow(data), (nrow(data))*.75)
job.test = Y[-train]
library(randomForest)
bag.job = randomForest(x=X, y=Y, subset = train, importance = TRUE, ntree=50)
source('C:/Users/Jeffrey Wu/Desktop/GitHub/Job-Match-Prediction/Tree.R')
error
source('C:/Users/Jeffrey Wu/Desktop/Research Project Materials/Lasso Quantile Regression Code.R')
lasso.mod = rq(y.net ~ ., data = barro, method = "lasso", lambda = bestlam, tau = 0.75)
lasso.coef=coef(lasso.mod)
lasso.coef[lasso.coef!=0]
###Quantile regression done with lasso
data(barro)
=======
X = shuffledData[,11:1814]
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
mean(cv.error)
job_data_v2 = read.csv("dtm_jobs_v2.csv", header =TRUE)
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
mean(cv.error)
Y = as.vector(shuffledData$Cat1)
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
mean(cv.error)
source('~/Documents/Git/Job-Match-Prediction/lasso.R')
job_data = read.csv("dtm_jobs_v2.csv", header =TRUE)
source('~/Documents/Git/Job-Match-Prediction/lasso.R')
source('~/Documents/Git/Job-Match-Prediction/lasso.R')
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
importance(rf.job, type =1)
import = importance(rf.job, type =1, class=1)
import = importance(rf.job, type =1, class="Admin")
View(import)
import = importance(rf.job, type =1, class="Business")
import = importance(rf.job, type =1, class="Tech")
import = importance(rf.job, type =1, class="Sales")
import = importance(rf.job, type =2, class="Sales")
import = importance(rf.job, type =1, class="Sales")
source('~/Documents/Git/Job-Match-Prediction/Tree.R')
source('~/Documents/Git/Job-Match-Prediction/lasso.R')
source('~/Documents/Git/Job-Match-Prediction/lasso.R')
coef(lasso.mod)
coef(lasso.mod)[!=0]
lasso.coef = coef(lasso.mod)
lasso.coef[lasso.coef!=0]
lasso.coef[!=0]
print(i in lasso.coef[i]!=0)
lasso.coef = coef(lasso.mod)
lasso.coef[lasso.coef != 0]
a = 0
for (i in length(lasso.coef)){
if (lass.coef != 0){
a = c(a,lasso.coef[i])
}
}
mean_error = mean(cv.error)
lasso.coef = coef(lasso.mod)
a = 0
for (i in length(lasso.coef)){
if (lass.coef != 0){
a = c(a,lasso.coef[i])
}
}
lasso.coef = coef(lasso.mod)
lasso.mod$beta
lasso.coef = lasso.mod$beta
View(lasso.coef)
View(lasso.coef)
lasso.coef = coef(lasso.mod)
lasso.coef = [which(lasso.coef!=0)]
lasso.coef = lasso.coef[which(lasso.coef!=0)]
lasso.coef = coef(lasso.mod,s=bestlam)
lasso.coef = lasso.coef[which(lasso.coef!=0)]
lasso.coef[1]
lasso.coef = coef(lasso.mod,s=bestlam)[1]
lasso.coef = lasso.coef[which(lasso.coef!=0)]
install.packages("glmnetcr")
library("glmnetcr")
nonzero.glmnet.cr(lasso.mod)
library("glmnetcr")
nonzero.glmnet.cr(lasso.mod)
library("glmnet")
nonzero.glmnet.cr(lasso.mod)
install.packages("coefplot")
library("coefplot")
extract.coef(lasso.mod)
extract.coef(lasso.mod, s=bestlam)
View(lasso.coef)
lasso.coef = coef(lasso.mod,s=bestlam)
View(lasso.coef)
lasso.coef
lasso.coef$Admin
admin = lasso.coef$Admin
View(admin)
admin[which(admin!=0)]
admin[admin!=0]
>>>>>>> 84fa75361c3cea7561078cba140fc1010675cc23
